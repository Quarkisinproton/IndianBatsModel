{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!pip install optuna librosa pyyaml pandas matplotlib torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a3cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Clone Repository\n",
    "!git clone https://github.com/Quarkisinproton/IndianBatsModel.git\n",
    "%cd IndianBatsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Patch Codebase (Fixes & Features)\n",
    "\n",
    "# A. Fix Syntax Error in whombat_project_to_wombat.py\n",
    "file_path = 'MainShitz/data_prep/whombat_project_to_wombat.py'\n",
    "try:\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    # Fix missing colon if present\n",
    "    bad_syntax = \"if not ann_list continue\"\n",
    "    good_syntax = \"if not ann_list: continue\"\n",
    "    if bad_syntax in content:\n",
    "        content = content.replace(bad_syntax, good_syntax)\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(content)\n",
    "        print(\"Fixed syntax error in whombat_project_to_wombat.py\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: {file_path} not found.\")\n",
    "\n",
    "# B. Patch train.py to report Final Validation Loss\n",
    "train_script_path = 'MainShitz/train.py'\n",
    "try:\n",
    "    with open(train_script_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    if \"FINAL_VAL_LOSS\" not in content:\n",
    "        target_str = \"print(f\\\"Training curves saved to {plot_path}\\\")\"\n",
    "        new_code = \"\"\"\n",
    "    print(f\"Training curves saved to {plot_path}\")\n",
    "\n",
    "    # Report final validation loss for hyperparameter tuning\n",
    "    if val_losses:\n",
    "        print(f\"FINAL_VAL_LOSS: {val_losses[-1]}\")\n",
    "\"\"\"\n",
    "        if target_str in content:\n",
    "            content = content.replace(target_str, new_code)\n",
    "            with open(train_script_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            print(\"Successfully patched train.py\")\n",
    "        else:\n",
    "            print(\"WARNING: Could not find target string to patch train.py.\")\n",
    "    else:\n",
    "        print(\"train.py already contains FINAL_VAL_LOSS reporting.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: {train_script_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create smart_tuner.py\n",
    "tuner_code = \"\"\"\n",
    "import optuna\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Suggest Hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    \n",
    "    print(f\"\\\\n--- Trial {trial.number} ---\")\n",
    "    print(f\"Params: lr={learning_rate}, bs={batch_size}, wd={weight_decay}\")\n",
    "\n",
    "    # 2. Load Base Config\n",
    "    base_config_path = 'configs/config.yaml'\n",
    "    if not os.path.exists(base_config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found: {base_config_path}\")\n",
    "        \n",
    "    with open(base_config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    if 'train' not in config:\n",
    "        config['train'] = {}\n",
    "        \n",
    "    config['train']['learning_rate'] = learning_rate\n",
    "    config['train']['batch_size'] = batch_size\n",
    "    config['train']['weight_decay'] = weight_decay\n",
    "    \n",
    "    model_save_path = os.path.join('models', f'trial_{trial.number}.pth')\n",
    "    config['train']['model_save_path'] = model_save_path\n",
    "    \n",
    "    temp_config_path = f'temp_config_{trial.number}.yaml'\n",
    "    with open(temp_config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "        \n",
    "    # 3. Run Training\n",
    "    cmd = [sys.executable, \"-m\", \"MainShitz.train\", \"--config\", temp_config_path]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        output = result.stdout\n",
    "        \n",
    "        final_val_loss = None\n",
    "        for line in output.splitlines():\n",
    "            if \"FINAL_VAL_LOSS:\" in line:\n",
    "                try:\n",
    "                    final_val_loss = float(line.split(\"FINAL_VAL_LOSS:\")[1].strip())\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        \n",
    "        if final_val_loss is None:\n",
    "            print(\"Warning: Could not find FINAL_VAL_LOSS in output.\")\n",
    "            return 999.0\n",
    "            \n",
    "        return final_val_loss\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Training failed for trial {trial.number}\")\n",
    "        print(\"Error:\", e.stderr)\n",
    "        return 999.0\n",
    "        \n",
    "    finally:\n",
    "        if os.path.exists(temp_config_path):\n",
    "            os.remove(temp_config_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    print(\"Starting Hyperparameter Optimization...\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*40)\n",
    "    print(\"Optimization Complete\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best Validation Loss: {study.best_value}\")\n",
    "    print(\"=\"*40)\n",
    "\"\"\"\n",
    "\n",
    "with open('smart_tuner.py', 'w') as f:\n",
    "    f.write(tuner_code)\n",
    "print(\"Created smart_tuner.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import librosa\n",
    "import sys\n",
    "\n",
    "# Ensure we are in the right directory for imports\n",
    "if os.getcwd().split('/')[-1] != 'IndianBatsModel':\n",
    "    if os.path.exists('IndianBatsModel'):\n",
    "        os.chdir('IndianBatsModel')\n",
    "    sys.path.append('.')\n",
    "\n",
    "from MainShitz.data_prep.wombat_to_spectrograms import process_all as generate_spectrograms\n",
    "from MainShitz.data_prep.whombat_project_to_wombat import convert_whombat_project_to_wombat_jsons\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1. Input Paths (Adjust these to match your Kaggle Dataset structure)\n",
    "#    These are the folders containing your .wav files\n",
    "RAW_AUDIO_DIRS = [\n",
    "    '/kaggle/input/annotations-tenuis-ceylonicus/Pip ceylonicus',\n",
    "    '/kaggle/input/annotations-tenuis-ceylonicus/Pip._tenuis'\n",
    "]\n",
    "\n",
    "#    These are the JSON exports from Whombat\n",
    "WHOMBAT_PROJECT_JSONS = [\n",
    "    '/kaggle/input/annotations-tenuis-ceylonicus/tenuis annotations.json',\n",
    "    '/kaggle/input/annotations-tenuis-ceylonicus/Pip ceylonicus.json',\n",
    "]\n",
    "\n",
    "#    Path to your noise data (UPDATE THIS if your folder name is different)\n",
    "NOISE_AUDIO_DIR = '/kaggle/input/noise-data' \n",
    "\n",
    "# 2. Output Paths (In the writable /kaggle/working directory)\n",
    "JSON_DIR = '/kaggle/working/data/annotations_json_folder'\n",
    "SPECT_OUT = '/kaggle/working/data/processed/spectrograms'\n",
    "\n",
    "os.makedirs(JSON_DIR, exist_ok=True)\n",
    "os.makedirs(SPECT_OUT, exist_ok=True)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "# 1. Convert Bat Annotations\n",
    "print(\"Converting Bat Annotations...\")\n",
    "for pj in WHOMBAT_PROJECT_JSONS:\n",
    "    # Fix: Only pass 2 arguments (Input File, Output Dir)\n",
    "    convert_whombat_project_to_wombat_jsons(pj, JSON_DIR)\n",
    "\n",
    "# 2. Generate Noise Annotations\n",
    "print(\"Generating Noise Annotations...\")\n",
    "noise_files = glob.glob(os.path.join(NOISE_AUDIO_DIR, \"*.wav\"))\n",
    "print(f\"Found {len(noise_files)} noise files.\")\n",
    "\n",
    "noise_annotations = []\n",
    "for nf in noise_files:\n",
    "    try:\n",
    "        # Handle different librosa versions for duration check\n",
    "        try:\n",
    "            dur = librosa.get_duration(path=nf)\n",
    "        except TypeError:\n",
    "            dur = librosa.get_duration(filename=nf)\n",
    "            \n",
    "        # Create a simple annotation for the whole file\n",
    "        ann = {\n",
    "            \"start\": 0.0,\n",
    "            \"end\": dur,\n",
    "            \"label\": \"Noise\",\n",
    "            \"filename\": os.path.basename(nf)\n",
    "        }\n",
    "        noise_annotations.append(ann)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {nf}: {e}\")\n",
    "\n",
    "noise_json_path = os.path.join(JSON_DIR, \"noise_annotations.json\")\n",
    "with open(noise_json_path, 'w') as f:\n",
    "    json.dump(noise_annotations, f, indent=4)\n",
    "\n",
    "# 3. Generate Spectrograms\n",
    "print(\"Generating Spectrograms...\")\n",
    "\n",
    "# Combine bat audio dirs and noise audio dir into one list for the processor\n",
    "ALL_AUDIO_DIRS = RAW_AUDIO\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import librosa\n",
    "import sys\n",
    "\n",
    "# Ensure we are in the right directory for imports\n",
    "if os.getcwd().split('/')[-1] != 'IndianBatsModel':\n",
    "    if os.path.exists('IndianBatsModel'):\n",
    "        os.chdir('IndianBatsModel')\n",
    "    sys.path.append('.')\n",
    "\n",
    "from MainShitz.data_prep.wombat_to_spectrograms import process_all as generate_spectrograms\n",
    "from MainShitz.data_prep.whombat_project_to_wombat import convert_whombat_project_to_wombat_jsons\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# 1. Input Paths (Adjust these to match your Kaggle Dataset structure)\n",
    "#    These are the folders containing your .wav files\n",
    "RAW_AUDIO_DIRS = [\n",
    "    '/kaggle/input/annotations-tenuis-ceylonicus/Pip ceylonicus',\n",
    "    '/kaggle/input/annotations-tenuis-ceylonicus/Pip._tenuis'\n",
    "]\n",
    "\n",
    "#    These are the JSON exports from Whombat\n",
    "WHOMBAT_PROJECT_JSONS = [\n",
    "    '/kaggle/input/annotations-tenuis-ceylonicus/tenuis annotations.json',\n",
    "    '/kaggle/input/annotations-tenuis-ceylonicus/Pip ceylonicus.json',\n",
    "]\n",
    "\n",
    "#    Path to your noise data (UPDATE THIS if your folder name is different)\n",
    "NOISE_AUDIO_DIR = '/kaggle/input/noice-files/Noise' \n",
    "\n",
    "# 2. Output Paths (In the writable /kaggle/working directory)\n",
    "JSON_DIR = '/kaggle/working/data/annotations_json_folder'\n",
    "SPECT_OUT = '/kaggle/working/data/processed/spectrograms'\n",
    "\n",
    "os.makedirs(JSON_DIR, exist_ok=True)\n",
    "os.makedirs(SPECT_OUT, exist_ok=True)\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "# 1. Convert Bat Annotations\n",
    "print(\"Converting Bat Annotations...\")\n",
    "for pj in WHOMBAT_PROJECT_JSONS:\n",
    "    # Fix: Only pass 2 arguments (Input File, Output Dir)\n",
    "    convert_whombat_project_to_wombat_jsons(pj, JSON_DIR)\n",
    "\n",
    "# 2. Generate Noise Annotations\n",
    "print(\"Generating Noise Annotations...\")\n",
    "noise_files = glob.glob(os.path.join(NOISE_AUDIO_DIR, \"*.wav\"))\n",
    "print(f\"Found {len(noise_files)} noise files.\")\n",
    "\n",
    "noise_annotations = []\n",
    "for nf in noise_files:\n",
    "    try:\n",
    "        # Handle different librosa versions for duration check\n",
    "        try:\n",
    "            dur = librosa.get_duration(path=nf)\n",
    "        except TypeError:\n",
    "            dur = librosa.get_duration(filename=nf)\n",
    "            \n",
    "        # Create a simple annotation for the whole file\n",
    "        ann = {\n",
    "            \"start\": 0.0,\n",
    "            \"end\": dur,\n",
    "            \"label\": \"Noise\",\n",
    "            \"filename\": os.path.basename(nf)\n",
    "        }\n",
    "        noise_annotations.append(ann)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {nf}: {e}\")\n",
    "\n",
    "noise_json_path = os.path.join(JSON_DIR, \"noise_annotations.json\")\n",
    "with open(noise_json_path, 'w') as f:\n",
    "    json.dump(noise_annotations, f, indent=4)\n",
    "\n",
    "# 3. Generate Spectrograms\n",
    "print(\"Generating Spectrograms...\")\n",
    "\n",
    "# Combine bat audio dirs and noise audio dir into one list for the processor\n",
    "ALL_AUDIO_DIRS = RAW_AUDIO_DIRS + [NOISE_AUDIO_DIR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Update Config and Run Tuner\n",
    "import yaml\n",
    "\n",
    "config_path = 'configs/config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update data path\n",
    "config['data']['processed_data_path'] = SPECT_OUT\n",
    "config['data']['train_spectrograms'] = SPECT_OUT\n",
    "\n",
    "# Set epochs for tuning (e.g., 5 epochs per trial)\n",
    "config['train']['epochs'] = 5\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"Config updated. Starting Tuner...\")\n",
    "\n",
    "!python smart_tuner.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
