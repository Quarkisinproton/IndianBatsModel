{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install Dependencies\n",
    "!pip install optuna librosa pyyaml pandas matplotlib torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a3cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Clone Repository\n",
    "!git clone https://github.com/Quarkisinproton/IndianBatsModel.git\n",
    "%cd IndianBatsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e9a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Patch Codebase (Fixes & Features)\n",
    "\n",
    "# A. Fix Syntax Error in whombat_project_to_wombat.py\n",
    "file_path = 'MainShitz/data_prep/whombat_project_to_wombat.py'\n",
    "try:\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    # Fix missing colon if present\n",
    "    bad_syntax = \"if not ann_list continue\"\n",
    "    good_syntax = \"if not ann_list: continue\"\n",
    "    if bad_syntax in content:\n",
    "        content = content.replace(bad_syntax, good_syntax)\n",
    "        with open(file_path, 'w') as f:\n",
    "            f.write(content)\n",
    "        print(\"Fixed syntax error in whombat_project_to_wombat.py\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: {file_path} not found.\")\n",
    "\n",
    "# B. Patch train.py to report Final Validation Loss\n",
    "train_script_path = 'MainShitz/train.py'\n",
    "try:\n",
    "    with open(train_script_path, 'r') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    if \"FINAL_VAL_LOSS\" not in content:\n",
    "        target_str = \"print(f\\\"Training curves saved to {plot_path}\\\")\"\n",
    "        new_code = \"\"\"\n",
    "    print(f\"Training curves saved to {plot_path}\")\n",
    "\n",
    "    # Report final validation loss for hyperparameter tuning\n",
    "    if val_losses:\n",
    "        print(f\"FINAL_VAL_LOSS: {val_losses[-1]}\")\n",
    "\"\"\"\n",
    "        if target_str in content:\n",
    "            content = content.replace(target_str, new_code)\n",
    "            with open(train_script_path, 'w') as f:\n",
    "                f.write(content)\n",
    "            print(\"Successfully patched train.py\")\n",
    "        else:\n",
    "            print(\"WARNING: Could not find target string to patch train.py.\")\n",
    "    else:\n",
    "        print(\"train.py already contains FINAL_VAL_LOSS reporting.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: {train_script_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5d21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create smart_tuner.py\n",
    "tuner_code = \"\"\"\n",
    "import optuna\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Suggest Hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    \n",
    "    print(f\"\\\\n--- Trial {trial.number} ---\")\n",
    "    print(f\"Params: lr={learning_rate}, bs={batch_size}, wd={weight_decay}\")\n",
    "\n",
    "    # 2. Load Base Config\n",
    "    base_config_path = 'configs/config.yaml'\n",
    "    if not os.path.exists(base_config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found: {base_config_path}\")\n",
    "        \n",
    "    with open(base_config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    if 'train' not in config:\n",
    "        config['train'] = {}\n",
    "        \n",
    "    config['train']['learning_rate'] = learning_rate\n",
    "    config['train']['batch_size'] = batch_size\n",
    "    config['train']['weight_decay'] = weight_decay\n",
    "    \n",
    "    model_save_path = os.path.join('models', f'trial_{trial.number}.pth')\n",
    "    config['train']['model_save_path'] = model_save_path\n",
    "    \n",
    "    temp_config_path = f'temp_config_{trial.number}.yaml'\n",
    "    with open(temp_config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "        \n",
    "    # 3. Run Training\n",
    "    cmd = [sys.executable, \"-m\", \"MainShitz.train\", \"--config\", temp_config_path]\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        output = result.stdout\n",
    "        \n",
    "        final_val_loss = None\n",
    "        for line in output.splitlines():\n",
    "            if \"FINAL_VAL_LOSS:\" in line:\n",
    "                try:\n",
    "                    final_val_loss = float(line.split(\"FINAL_VAL_LOSS:\")[1].strip())\n",
    "                except ValueError:\n",
    "                    pass\n",
    "        \n",
    "        if final_val_loss is None:\n",
    "            print(\"Warning: Could not find FINAL_VAL_LOSS in output.\")\n",
    "            return 999.0\n",
    "            \n",
    "        return final_val_loss\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Training failed for trial {trial.number}\")\n",
    "        print(\"Error:\", e.stderr)\n",
    "        return 999.0\n",
    "        \n",
    "    finally:\n",
    "        if os.path.exists(temp_config_path):\n",
    "            os.remove(temp_config_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    print(\"Starting Hyperparameter Optimization...\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*40)\n",
    "    print(\"Optimization Complete\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"Best Validation Loss: {study.best_value}\")\n",
    "    print(\"=\"*40)\n",
    "\"\"\"\n",
    "\n",
    "with open('smart_tuner.py', 'w') as f:\n",
    "    f.write(tuner_code)\n",
    "print(\"Created smart_tuner.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791a133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Data Preparation\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from MainShitz.data_prep.wombat_to_spectrograms import process_all as generate_spectrograms\n",
    "from MainShitz.data_prep.whombat_project_to_wombat import convert_whombat_project_to_wombat_jsons\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import librosa\n",
    "import json\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Adjust these paths to match your Kaggle Dataset\n",
    "NOISE_AUDIO_DIR = '/kaggle/input/noise-data' # <--- UPDATE THIS PATH\n",
    "RAW_AUDIO_DIRS = [\n",
    "    '/kaggle/input/pip-ceylonicusbat-species',\n",
    "    '/kaggle/input/pip-tenuisbat-species',\n",
    "    NOISE_AUDIO_DIR\n",
    "]\n",
    "WHOMBAT_PROJECT_JSONS = [\n",
    "    '/kaggle/input/pip-tenuisbat-species/tenuis annotations.json',\n",
    "    '/kaggle/input/pip-ceylonicusbat-species/Pip ceylonicus.json',\n",
    "]\n",
    "\n",
    "WORK_DIR = '/kaggle/working'\n",
    "JSON_DIR = os.path.join(WORK_DIR, 'data/annotations_json_folder')\n",
    "SPECT_OUT = os.path.join(WORK_DIR, 'data/processed/spectrograms')\n",
    "\n",
    "Path(JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(SPECT_OUT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Convert Annotations (Bats)\n",
    "print(\"Converting Bat Annotations...\")\n",
    "for pj in WHOMBAT_PROJECT_JSONS:\n",
    "    convert_whombat_project_to_wombat_jsons(pj, JSON_DIR, RAW_AUDIO_DIRS)\n",
    "\n",
    "# 2. Generate Noise Annotations\n",
    "print(\"Generating Noise Annotations...\")\n",
    "if os.path.exists(NOISE_AUDIO_DIR):\n",
    "    noise_files = glob.glob(os.path.join(NOISE_AUDIO_DIR, \"*.wav\"))\n",
    "    print(f\"Found {len(noise_files)} noise files.\")\n",
    "    for nf in noise_files:\n",
    "        try:\n",
    "            duration = librosa.get_duration(path=nf)\n",
    "            fname = os.path.basename(nf)\n",
    "            entry = {\n",
    "                \"recording\": fname,\n",
    "                \"annotations\": [{\n",
    "                    \"start_time\": 0.0,\n",
    "                    \"end_time\": duration,\n",
    "                    \"label\": \"Noise\"\n",
    "                }]\n",
    "            }\n",
    "            # Save individual JSON\n",
    "            json_name = os.path.splitext(fname)[0] + \".json\"\n",
    "            with open(os.path.join(JSON_DIR, json_name), 'w') as f:\n",
    "                json.dump(entry, f, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing noise file {nf}: {e}\")\n",
    "else:\n",
    "    print(f\"Warning: Noise directory {NOISE_AUDIO_DIR} not found. Skipping noise generation.\")\n",
    "\n",
    "# 3. Generate Spectrograms\n",
    "print(\"Generating Spectrograms...\")\n",
    "generate_spectrograms(JSON_DIR, SPECT_OUT)\n",
    "\n",
    "print(\"Data Prep Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440af30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Update Config and Run Tuner\n",
    "import yaml\n",
    "\n",
    "config_path = 'configs/config.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Update data path\n",
    "config['data']['processed_data_path'] = SPECT_OUT\n",
    "config['data']['train_spectrograms'] = SPECT_OUT\n",
    "\n",
    "# Set epochs for tuning (e.g., 5 epochs per trial)\n",
    "config['train']['epochs'] = 5\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"Config updated. Starting Tuner...\")\n",
    "\n",
    "!python smart_tuner.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
