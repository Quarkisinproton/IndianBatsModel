{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndianBatsModel - Real-World Inference Pipeline\n",
    "\n",
    "This notebook runs the trained model on **raw, uncurated audio files**.\n",
    "Unlike the test notebook, this pipeline handles:\n",
    "1.  **Detection**: Finding potential bat calls in long recordings (ignoring silence/noise).\n",
    "2.  **Classification**: Predicting the species for each detected call.\n",
    "3.  **Filtering**: Ignoring low-confidence predictions.\n",
    "\n",
    "**Use this for:** Field recordings or files where you don't know if/where the bats are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Environment\n",
    "!git clone https://github.com/Quarkisinproton/IndianBatsModel.git\n",
    "!pip install librosa pyyaml pandas matplotlib scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Modules\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define working directory\n",
    "WORK_DIR = '/kaggle/working'\n",
    "\n",
    "\n",
    "# Add repo path\n",
    "REPO_DIR = os.path.join(WORK_DIR, 'IndianBatsModel')\n",
    "SRC_DIR = os.path.join(REPO_DIR, 'src')\n",
    "if REPO_DIR not in sys.path: sys.path.append(REPO_DIR)\n",
    "if SRC_DIR not in sys.path: sys.path.append(SRC_DIR)\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    from src.models.cnn_with_features import CNNWithFeatures\n",
    "    from src.data_prep.wombat_to_spectrograms import make_mel_spectrogram\n",
    "    from src.data_prep.extract_end_frequency import compute_end_frequency\n",
    "    print(\"Imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuration\n",
    "\n",
    "# --- INPUTS ---\n",
    "# Path to your trained model\n",
    "MODEL_PATH = '/kaggle/working/models/bat_fused_best.pth'\n",
    "\n",
    "# Path to folder containing RAW .wav files\n",
    "INPUT_AUDIO_DIR = '/kaggle/input/your-test-audio-folder' \n",
    "\n",
    "# Auto-detect input audio directory if default doesn't exist\n",
    "if not os.path.exists(INPUT_AUDIO_DIR):\n",
    "    print(f\"Default input directory {INPUT_AUDIO_DIR} not found. Searching /kaggle/input...\")\n",
    "    for root, dirs, files in os.walk('/kaggle/input'):\n",
    "        if any(f.lower().endswith(('.wav', '.mp3', '.flac')) for f in files):\n",
    "            INPUT_AUDIO_DIR = root\n",
    "            print(f\"Found audio files in: {INPUT_AUDIO_DIR}\")\n",
    "            break\n",
    "\n",
    "# --- DETECTION SETTINGS ---\n",
    "SAMPLE_RATE = 250000  # Typical for bat recorders (adjust if needed)\n",
    "MIN_FREQ = 15000      # High-pass filter cutoff (15kHz) to remove wind/noise\n",
    "ENERGY_THRESH = 0.02  # RMS energy threshold to trigger detection\n",
    "MIN_DURATION = 0.01   # Minimum call duration (seconds)\n",
    "PAD_DURATION = 0.05   # Padding around detected call (seconds)\n",
    "CONFIDENCE_THRESH = 70.0 # Only report predictions > 70% confidence\n",
    "\n",
    "# --- MODEL SETTINGS ---\n",
    "NUM_CLASSES = 2       # Must match your trained model\n",
    "CLASS_NAMES = ['pip-ceylonicusbat-species', 'pip-tenuisbat-species'] # Ensure correct order!\n",
    "\n",
    "# Auto-detect model in /kaggle/input\n",
    "found_models = []\n",
    "for root, dirs, files in os.walk('/kaggle/input'):\n",
    "    for file in files:\n",
    "        if file.endswith('.pth'):\n",
    "            found_models.append(os.path.join(root, file))\n",
    "if found_models:\n",
    "    MODEL_PATH = found_models[0]\n",
    "    print(f\"Auto-detected model: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Detection & Processing Functions\n",
    "\n",
    "def detect_events(y, sr, min_freq=15000, threshold=0.01, min_dur=0.01, pad=0.05):\n",
    "    \"\"\"Finds segments of interest in the audio based on energy in high frequencies.\"\"\"\n",
    "    # 1. High-pass filter\n",
    "    # Simple approach: Compute STFT, zero out low bins, reconstruct (or just use spectral energy)\n",
    "    S = librosa.stft(y, n_fft=2048, hop_length=512)\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=2048)\n",
    "    \n",
    "    # Zero out frequencies below min_freq\n",
    "    S_filtered = S.copy()\n",
    "    mask = freqs < min_freq\n",
    "    S_filtered[mask, :] = 0\n",
    "    \n",
    "    # 2. Calculate RMS energy profile of filtered signal\n",
    "    rms = librosa.feature.rms(S=S_filtered, frame_length=2048, hop_length=512)[0]\n",
    "    times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=512)\n",
    "    \n",
    "    # 3. Thresholding\n",
    "    is_active = rms > threshold\n",
    "    \n",
    "    # 4. Group into segments\n",
    "    events = []\n",
    "    start = None\n",
    "    for i, active in enumerate(is_active):\n",
    "        if active and start is None:\n",
    "            start = times[i]\n",
    "        elif not active and start is not None:\n",
    "            end = times[i]\n",
    "            if (end - start) >= min_dur:\n",
    "                events.append((max(0, start - pad), min(librosa.get_duration(y=y, sr=sr), end + pad)))\n",
    "            start = None\n",
    "            \n",
    "    # Handle case where file ends while active\n",
    "    if start is not None:\n",
    "        end = times[-1]\n",
    "        if (end - start) >= min_dur:\n",
    "            events.append((max(0, start - pad), end))\n",
    "            \n",
    "    # Merge overlapping segments\n",
    "    if not events: return []\n",
    "    \n",
    "    merged = []\n",
    "    curr_start, curr_end = events[0]\n",
    "    for next_start, next_end in events[1:]:\n",
    "        if next_start <= curr_end:\n",
    "            curr_end = max(curr_end, next_end)\n",
    "        else:\n",
    "            merged.append((curr_start, curr_end))\n",
    "            curr_start, curr_end = next_start, next_end\n",
    "    merged.append((curr_start, curr_end))\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def prepare_input(y, sr, start, end):\n",
    "    \"\"\"Extracts segment, generates spectrogram and features for the model.\"\"\"\n",
    "    # Extract audio\n",
    "    start_sample = int(start * sr)\n",
    "    end_sample = int(end * sr)\n",
    "    y_seg = y[start_sample:end_sample]\n",
    "    \n",
    "    if len(y_seg) < 512: return None, None, None, None # Too short\n",
    "    \n",
    "    # 1. Spectrogram\n",
    "    S_db = make_mel_spectrogram(y_seg, sr)\n",
    "    \n",
    "    # Convert to Image (normalize to 0-255 like training)\n",
    "    # Normalize to 0-1\n",
    "    S_min, S_max = S_db.min(), S_db.max()\n",
    "    S_norm = (S_db - S_min) / (S_max - S_min + 1e-8)\n",
    "    \n",
    "    # Apply Magma Colormap (matches training data)\n",
    "    S_colored = plt.get_cmap('magma')(S_norm)\n",
    "    \n",
    "    # Convert to uint8 RGB (drop alpha)\n",
    "    S_img = (S_colored[:, :, :3] * 255).astype(np.uint8)\n",
    "    \n",
    "    # Flip vertically so low frequency is at the bottom\n",
    "    S_img = np.flipud(S_img)\n",
    "    \n",
    "    img = Image.fromarray(S_img)\n",
    "    \n",
    "    # Transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(img)\n",
    "    \n",
    "    # 2. Features (End Frequency)\n",
    "    end_freq = compute_end_frequency(y, sr, start, end)\n",
    "    if np.isnan(end_freq):\n",
    "        end_freq = 0.0\n",
    "    feat_tensor = torch.tensor([end_freq], dtype=torch.float32)\n",
    "    \n",
    "    return img_tensor, feat_tensor, img, y_seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Initialize model structure\n",
    "    model = CNNWithFeatures(num_classes=NUM_CLASSES, numeric_feat_dim=1, pretrained=False)\n",
    "    \n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(f\"Loading {MODEL_PATH}...\")\n",
    "        checkpoint = torch.load(MODEL_PATH, map_location=device, weights_only=False)\n",
    "        \n",
    "        if isinstance(checkpoint, torch.nn.Module):\n",
    "            model = checkpoint\n",
    "        elif isinstance(checkpoint, dict):\n",
    "            if 'model_state_dict' in checkpoint:\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(\"Model loaded!\")\n",
    "    else:\n",
    "        print(\"Model file not found!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Run Inference Loop\n",
    "from IPython.display import display\n",
    "\n",
    "# Create output directory for spectrograms\n",
    "SPECTROGRAM_DIR = os.path.join(WORK_DIR, 'inference_spectrograms')\n",
    "os.makedirs(SPECTROGRAM_DIR, exist_ok=True)\n",
    "print(f'Saving spectrograms to {SPECTROGRAM_DIR}...')\n",
    "\n",
    "results = []\n",
    "\n",
    "# Find audio files\n",
    "audio_files = []\n",
    "if os.path.exists(INPUT_AUDIO_DIR):\n",
    "    for root, dirs, files in os.walk(INPUT_AUDIO_DIR):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(('.wav', '.mp3', '.flac')):\n",
    "                audio_files.append(os.path.join(root, f))\n",
    "else:\n",
    "    print(f\"Input directory {INPUT_AUDIO_DIR} does not exist.\")\n",
    "\n",
    "print(f\"Found {len(audio_files)} files to process.\")\n",
    "\n",
    "for audio_path in tqdm(audio_files):\n",
    "    try:\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(audio_path, sr=None)\n",
    "        \n",
    "        # Detect events\n",
    "        events = detect_events(y, sr, min_freq=MIN_FREQ, threshold=ENERGY_THRESH)\n",
    "        \n",
    "        if not events:\n",
    "            results.append({\n",
    "                'filename': os.path.basename(audio_path),\n",
    "                'start': '-', 'end': '-',\n",
    "                'prediction': 'No Detection',\n",
    "                'confidence': 0.0\n",
    "            })\n",
    "            continue\n",
    "            \n",
    "        # Process each event\n",
    "        for start, end in events:\n",
    "            img_t, feat_t, pil_img, y_seg = prepare_input(y, sr, start, end)\n",
    "            if img_t is None: continue\n",
    "            \n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                img_batch = img_t.unsqueeze(0).to(device)\n",
    "                feat_batch = feat_t.unsqueeze(0).to(device)\n",
    "                \n",
    "                output = model(img_batch, feat_batch)\n",
    "                probs = torch.nn.functional.softmax(output, dim=1)\n",
    "                conf, pred_idx = torch.max(probs, 1)\n",
    "                \n",
    "            confidence_pct = conf.item() * 100\n",
    "            pred_class = CLASS_NAMES[pred_idx.item()]\n",
    "            \n",
    "            # Filter low confidence\n",
    "            final_pred = pred_class if confidence_pct >= CONFIDENCE_THRESH else \"Uncertain\"\n",
    "            \n",
    "            # Display inline if confident\n",
    "            if final_pred != \"Uncertain\":\n",
    "                print(f\"\\nDetected: {final_pred} ({confidence_pct:.1f}%) at {start:.2f}-{end:.2f}s in {os.path.basename(audio_path)}\")\n",
    "                # Show with axes\n",
    "                # Generate high-res spectrogram for display\n",
    "                plt.figure(figsize=(10, 4))\n",
    "                # Compute STFT (Linear Frequency)\n",
    "                D = librosa.amplitude_to_db(np.abs(librosa.stft(y_seg)), ref=np.max)\n",
    "                librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='linear', cmap='magma')\n",
    "                plt.colorbar(format='%+2.0f dB')\n",
    "                plt.title(f'{final_pred} ({confidence_pct:.1f}%)')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Save Spectrogram\n",
    "                spec_filename = f\"{os.path.basename(audio_path)}_{start:.2f}_{end:.2f}_{final_pred}.png\"\n",
    "                spec_path = os.path.join(SPECTROGRAM_DIR, spec_filename)\n",
    "                pil_img.save(spec_path)\n",
    "\n",
    "            results.append({\n",
    "                'filename': os.path.basename(audio_path),\n",
    "                'start': f\"{start:.2f}\",\n",
    "                'end': f\"{end:.2f}\",\n",
    "                'prediction': final_pred,\n",
    "                'confidence': f\"{confidence_pct:.1f}\"\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "# Display Results\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\nInference Results:\")\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Calculate Accuracy (If Ground Truth is known)\n",
    "# This assumes the filename contains the true species name (e.g., 'recording_speciesA_01.wav')\n",
    "\n",
    "if not df.empty and 'prediction' in df.columns:\n",
    "    # Define mapping from filename keywords to true labels\n",
    "    # UPDATE THIS based on your file naming convention\n",
    "    keyword_map = {\n",
    "        'ceylonicus': 'pip-ceylonicusbat-species',\n",
    "        'tenuis': 'pip-tenuisbat-species'\n",
    "    }\n",
    "    \n",
    "    def get_true_label(filename):\n",
    "        for key, label in keyword_map.items():\n",
    "            if key.lower() in filename.lower():\n",
    "                return label\n",
    "        return 'unknown'\n",
    "\n",
    "    df['true_label'] = df['filename'].apply(get_true_label)\n",
    "    \n",
    "    # Filter out 'No Detection' and 'Uncertain' for accuracy calc\n",
    "    valid_preds = df[\n",
    "        (df['prediction'] != 'No Detection') & \n",
    "        (df['prediction'] != 'Uncertain') & \n",
    "        (df['true_label'] != 'unknown')\n",
    "    ].copy()\n",
    "    \n",
    "    if not valid_preds.empty:\n",
    "        valid_preds['correct'] = valid_preds['prediction'] == valid_preds['true_label']\n",
    "        accuracy = valid_preds['correct'].mean() * 100\n",
    "        \n",
    "        print(f\"\\nAccuracy on {len(valid_preds)} confident detections: {accuracy:.1f}%\")\n",
    "        print(valid_preds[['filename', 'prediction', 'true_label', 'correct']].head(20))\n",
    "    else:\n",
    "        print(\"\\nCould not calculate accuracy (no valid matching predictions found).\")\n",
    "else:\n",
    "    print(\"No results to evaluate.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}