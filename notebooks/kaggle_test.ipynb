{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IndianBatsModel - Testing & Inference\n",
    "\n",
    "This notebook tests the trained Bat Species Classifier on new audio files.\n",
    "\n",
    "**Prerequisites:**\n",
    "1.  **Trained Model**: You must have a trained `.pth` model file (e.g., from the training notebook).\n",
    "2.  **Test Data**: Audio files organized in folders by species (similar to training data).\n",
    "\n",
    "**Steps:**\n",
    "1.  Setup Environment (Clone code).\n",
    "2.  Prepare Test Data (Generate Spectrograms).\n",
    "3.  Load Model.\n",
    "4.  Evaluate Accuracy.\n",
    "5.  Run Inference on individual files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Environment\n",
    "!git clone https://github.com/Quarkisinproton/IndianBatsModel.git\n",
    "!pip install librosa pyyaml pandas matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Modules\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Add repo path\n",
    "REPO_DIR = '/kaggle/working/IndianBatsModel'\n",
    "SRC_DIR = os.path.join(REPO_DIR, 'src')\n",
    "if REPO_DIR not in sys.path: sys.path.append(REPO_DIR)\n",
    "if SRC_DIR not in sys.path: sys.path.append(SRC_DIR)\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    from src.data_prep.generate_annotations import generate_annotations\n",
    "    from src.data_prep.wombat_to_spectrograms import process_all as generate_spectrograms\n",
    "    from src.data_prep.extract_end_frequency import process_all_and_write_csv as extract_features\n",
    "    from src.datasets.spectrogram_with_features_dataset import SpectrogramWithFeaturesDataset\n",
    "    from src.models.cnn_with_features import CNNWithFeatures\n",
    "    print(\"Imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Import Error: {e}\")\n",
    "    # Fallback imports\n",
    "    from data_prep.generate_annotations import generate_annotations\n",
    "    from data_prep.wombat_to_spectrograms import process_all as generate_spectrograms\n",
    "    from data_prep.extract_end_frequency import process_all_and_write_csv as extract_features\n",
    "    from datasets.spectrogram_with_features_dataset import SpectrogramWithFeaturesDataset\n",
    "    from models.cnn_with_features import CNNWithFeatures\n",
    "    print(\"Imports successful (fallback)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuration\n",
    "WORK_DIR = '/kaggle/working'\n",
    "\n",
    "# --- INPUTS ---\n",
    "# Path to your trained model file (Upload this to Kaggle Datasets if needed)\n",
    "# If you just ran training, it might be at: '/kaggle/working/models/bat_fused_best.pth'\n",
    "MODEL_PATH = '/kaggle/working/models/bat_fused_best.pth' \n",
    "\n",
    "# Path to TEST audio folders\n",
    "# (You can use the same folders as training to verify, or new folders for testing)\n",
    "TEST_AUDIO_DIRS = [\n",
    "    '/kaggle/input/pip-ceylonicusbat-species',\n",
    "    '/kaggle/input/pip-tenuisbat-species'\n",
    "]\n",
    "\n",
    "# --- OUTPUTS ---\n",
    "TEST_JSON_DIR = os.path.join(WORK_DIR, 'test_data/annotations')\n",
    "TEST_SPECT_DIR = os.path.join(WORK_DIR, 'test_data/spectrograms')\n",
    "TEST_FEATURES_DIR = os.path.join(WORK_DIR, 'test_data/features')\n",
    "TEST_FEATURES_CSV = os.path.join(TEST_FEATURES_DIR, 'test_features.csv')\n",
    "\n",
    "# Ensure directories exist\n",
    "Path(TEST_FEATURES_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Model Path: {MODEL_PATH}\")\n",
    "print(f\"Test Data Output: {TEST_SPECT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prepare Test Data\n",
    "# We need to convert the raw test audio into spectrograms and features, just like training.\n",
    "\n",
    "print(\"--- Step 1: Generating Annotations ---\")\n",
    "generate_annotations(\n",
    "    raw_audio_dirs=TEST_AUDIO_DIRS,\n",
    "    output_dir=TEST_JSON_DIR,\n",
    "    label_strategy='folder'\n",
    ")\n",
    "\n",
    "print(\"\\n--- Step 2: Generating Spectrograms ---\")\n",
    "generate_spectrograms(\n",
    "    raw_audio_dirs=TEST_AUDIO_DIRS,\n",
    "    json_dir=TEST_JSON_DIR,\n",
    "    out_dir=TEST_SPECT_DIR,\n",
    "    species_key='label'\n",
    ")\n",
    "\n",
    "print(\"\\n--- Step 3: Extracting Features ---\")\n",
    "extract_features(\n",
    "    raw_audio_dirs=TEST_AUDIO_DIRS,\n",
    "    json_dir=TEST_JSON_DIR,\n",
    "    out_csv=TEST_FEATURES_CSV,\n",
    "    species_key='label'\n",
    ")\n",
    "print(\"\\nTest Data Preparation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load Test Dataset\n",
    "try:\n",
    "    test_dataset = SpectrogramWithFeaturesDataset(\n",
    "        root_dir=TEST_SPECT_DIR,\n",
    "        features_csv=TEST_FEATURES_CSV\n",
    "    )\n",
    "    print(f\"Loaded Test Dataset: {len(test_dataset)} samples\")\n",
    "    print(f\"Classes: {test_dataset.class_to_idx}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Did spectrogram generation fail?\")\n",
    "    test_dataset = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Load Trained Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if len(test_dataset) > 0:\n",
    "    # Infer dimensions from dataset\n",
    "    sample_img, sample_feat, _ = test_dataset[0]\n",
    "    NUM_CLASSES = 3 # Adjust if your model was trained with different classes\n",
    "    FEAT_DIM = sample_feat.shape[0]\n",
    "    \n",
    "    print(f\"Initializing model with num_classes={NUM_CLASSES}, feat_dim={FEAT_DIM}\")\n",
    "    \n",
    "    model = CNNWithFeatures(num_classes=NUM_CLASSES, numeric_feat_dim=FEAT_DIM, pretrained=False)\n",
    "    \n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        try:\n",
    "            state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.to(device)\n",
    "            model.eval()\n",
    "            print(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model weights: {e}\")\n",
    "    else:\n",
    "        print(f\"CRITICAL: Model file not found at {MODEL_PATH}\")\n",
    "        print(\"Please upload your trained model or check the path.\")\n",
    "else:\n",
    "    print(\"Cannot load model: Dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Evaluate Accuracy\n",
    "if len(test_dataset) > 0 and 'model' in locals():\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"Running evaluation...\")\n",
    "    with torch.no_grad():\n",
    "        for images, features, labels in test_loader:\n",
    "            images, features = images.to(device), features.to(device)\n",
    "            outputs = model(images, features)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    class_names = list(test_dataset.class_to_idx.keys())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"CLASSIFICATION REPORT\")\n",
    "    print(\"=\"*40)\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    print(\"\\nCONFUSION MATRIX:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "else:\n",
    "    print(\"Skipping evaluation (missing model or data).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Make a Guess (Inference on Random Samples)\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "if len(test_dataset) > 0 and 'model' in locals():\n",
    "    # Pick 3 random samples\n",
    "    indices = random.sample(range(len(test_dataset)), min(3, len(test_dataset)))\n",
    "    \n",
    "    model.eval()\n",
    "    fig, axes = plt.subplots(1, len(indices), figsize=(15, 5))\n",
    "    if len(indices) == 1: axes = [axes]\n",
    "    \n",
    "    idx_to_class = {v: k for k, v in test_dataset.class_to_idx.items()}\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img, feat, label = test_dataset[idx]\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            img_batch = img.unsqueeze(0).to(device)\n",
    "            feat_batch = feat.unsqueeze(0).to(device)\n",
    "            output = model(img_batch, feat_batch)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            conf, pred_idx = torch.max(probs, 1)\n",
    "            \n",
    "        pred_class = idx_to_class[pred_idx.item()]\n",
    "        true_class = idx_to_class[label.item()]\n",
    "        confidence = conf.item() * 100\n",
    "        \n",
    "        # Plot\n",
    "        # Un-normalize for display\n",
    "        img_disp = img.permute(1, 2, 0).numpy()\n",
    "        img_disp = img_disp * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
    "        img_disp = np.clip(img_disp, 0, 1)\n",
    "        \n",
    "        axes[i].imshow(img_disp)\n",
    "        axes[i].set_title(f\"True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.1f}%\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping inference demo.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}